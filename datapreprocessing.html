<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>PH Twitter Fake News Analysis</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/prism.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="index.html">Home</a></li>
					<li><a href="#overview">Overview</a></li>
					<li><a href="#preproc">Preprocessing</a></li>
					<li><a href="#nlp">NLP</a></li>
					<li><a href="#tsa">TSA</a></li>
					<li><a href="#team">Team</a></li>
				</ul>
			</nav>

		<!-- Top -->
			<article id="overview" class="wrapper style1">
				<div class="container">
					<div class="row aln-left">
						<div class="col-12-medium">
							<header>
								<h1>This is the <strong>preprocessing</strong> step.</h1>
							</header>
							<p style="text-align: left;">Featured in this page are the steps taken by our group to preprocess the data that we have gathered. After reviewing the collected tweets, the team had to clean the dataset and make sure the formatting for each column is consistent. Plots were also used to see the trends in the data. A more detailed look in the process is explained in the next sections below.</p>
							<p style="text-align: left;">Here are the steps tackled in the next sections:</p>
							<ul style="list-style-type:none; margin-top:-30px; text-align: left;">
							  <li>ðŸ“ŒPreprocessing</li>
							  <li>ðŸ“ŒNatural Language Processing</li>
							  <li>ðŸ“ŒTime Series Analysis</li>
							</ul>
						</div>
					</div>
				</div>
			</article>

			<!-- Methods -->
			<article id="preproc" class="wrapper style3">
				<div class="container">
					<header>
						<h2>Let's talk about our data science methodology.</h2>
						<p>These are the steps we took to preprocess our dataset for data exploration, in this section we will explain how we handled missing values and outliers, how we ensured format consistency in the columns, how we normalized the data, and how we did categorical data encoding.</p>
					</header>
					<div class="row aln-left" style="text-align: left;">
						<div> <!--class="col-6 col-6-medium col-12-small"-->
                            <p>The first thing we did after loading the dataset is to <strong>deal with the null/missing values</strong>; we did this by using dropna to remove all the empty rows. We depicted empty rows as the rows with empty â€˜Timestampâ€™ column. Since the Timestamp marks the collection of a tweet, if there is no value in the Timestamp column, that means that there is no tweet collected in that row.</p>
							<p>We also removed the optional columns that would not be necessary for our study. Then we reset the index to match the number of rows. This step ensures that no errors in the next parts will occur because of null values. Lastly, all trailing whitespace in the all the column values were stripped.</p>
                        </div>
						<pre>
						<code class="language-python">
							import pandas as pd
							import matplotlib.pyplot as plt
							import matplotlib.dates as mdates

							# Read the combined csv file of the two datasheets
							df = pd.read_csv('dataset.csv')

							# Removing all rows that do not have content (those with ID starting 0)
							df = df.dropna(subset=['Timestamp'])

							# Removing optional columns
							df = df.drop(['Tweet Translated', 'Screenshot', 'Quote Tweets', 'Views', 'Rating', 'Remarks', 'Reviewer', 'Review'], axis=1


							# Resetting the index to match the number of rows
							df.reset_index(drop=True, inplace=True)

							# Remove trailing whitespace in all columns
							df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
						</code>
						</pre>

                        <div>
                            <p>To <strong>ensure format consistency</strong>, we had to convert each data type into the proper format (let alone for string data since we will be using NLP to format them).</p>
							<p>For <i>dates</i>, we had to convert each row in the 'Date posted' and 'Joined' column to datetime format. We did this by making a function that manually converts the values to the right format.</p>
							<p>The formatted values for the 'Date posted' column are stored in the 'Date posted (formatted)' column.</p>                                                
                        </div>

						<pre>
							<code class="language-python">
								# Convert each row in the date posted column to datetime format.
								def convert(x):
									datetime = x.split()
									date = datetime[0].split("/")
									if (len(date[1])<2):
									date[1] = "0" + date[1]
									if (len(date[2]) < 4):
									date[2] = "20" + date[2]
									fdate = date[2]+"-" + date[1] + "-" + date[0]
									time = datetime[1]
									tsplit = time.split(":")
									hour = tsplit[0]
									min = tsplit[1]
									if (len(hour) < 2):
									hour = "0" + hour
									ftime = hour + ":" + min + ":" + "00"
									fdatetime = fdate + "T" + ftime
									return pd.to_datetime(fdatetime)
								
								df['Date posted (formatted)'] = df['Date posted'].apply(lambda x: convert(x))
							</code>
						</pre>
                        <div>
							<p>Likewise, the formatted values for the 'Joined' column are then stored in the 'Date joined (formatted)' column.</p>
                        </div>
						<pre>
							<code class="language-python">
								# Convert each row in the joined column to datetime format.
								def convert(x):
									date = x.split("/")
									if (len(date[0])<2):
									date[0] = "0" + date[0]
									if (len(date[1]) < 4):
									date[1] = "20" + date[1]
									fdate = date[1] + "-" + date[0]
									return fdate
								
								df['Date joined (formatted)'] = df['Joined'].apply(lambda x: convert(x))
							</code>
						</pre>
                        <div>
							<p>For <i>numbers</i>, we had to ensure that all they were in float data type. We did this by making a function that checks if the data is in float format and converts it if not
								From exploring the data, we were able to find out that some numerical data in the thousands were written with K instead of the actual number. To retain the float format, we had to convert it.</p>
                        </div>
						<pre>
							<code class="language-python">
								def convert(x):
								if type(x) is float:
								  return x
								elif x.endswith('K'):
								  x = float(x[:-1])
								  x = x*1000
								else:
								  x = float(x)
								return x
							  
							  df['Likes'] = df['Likes'].apply(lambda x: convert(x))
							  df['Replies'] = df['Replies'].apply(lambda x: convert(x))
							  df['Retweets'] = df['Retweets'].apply(lambda x: convert(x))
							  df['Followers'] = df['Followers'].apply(lambda x: convert(x))
							  df['Following'] = df['Following'].apply(lambda x: convert(x))								
							</code>
						</pre>					
						<div>
							<p>After formatting, we <strong>handled the outliers</strong> that would throw off the results of our data. </p>
							<p>Since the scope of our data should only span from 2016 to 2022, we considered the data out of scope as outliers and removed them from the dataset.</p>
						</div>
						<pre>
							<code class="language-python">
								# Handle outliers for out of scope date
								df = df.loc[(df['Date posted (formatted)'] >= '2015-01-01') & (df['Date posted (formatted)'] < '2023-01-01')]
								df.reset_index(drop=True, inplace=True)
					
							</code>
						</pre>

						<div>
							<p>We also found out that there were outliers in the 'Followers' and 'Following' column using the Interquartile Range (IQR). As to avoid disrupting the original data, we copied the contents into test_df and explored the outliers there.
								We decided not to remove these outliers since the huge gap was essential in analyzing the other features.</p>
						</div>

						<pre>
							<code class="language-python">
								# Handle outliers for followers (dapat after formatting ito)
								import numpy as np
								
								test_df = df.copy()
								# IQR for Followers
								# Calculate the upper and lower limits
								Q1 = test_df['Followers'].quantile(0.25)
								Q3 = test_df['Followers'].quantile(0.75)
								IQR = Q3 - Q1
								lower = Q1 - 1.5*IQR
								upper = Q3 + 1.5*IQR
								 
								# Create arrays of Boolean values indicating the outlier rows
								upper_array = np.where(test_df['Followers']>=upper)[0]
								lower_array = np.where(test_df['Followers']<=lower)[0]
								
								print('upper bound:\n', test_df.loc[upper_array, 'Followers'])
								print('lower bound:\n', test_df.loc[lower_array, 'Followers'])
								
								# IQR for Following
								# Calculate the upper and lower limits
								Q1 = test_df['Following'].quantile(0.25)
								Q3 = test_df['Following'].quantile(0.75)
								IQR = Q3 - Q1
								lower = Q1 - 1.5*IQR
								upper = Q3 + 1.5*IQR
								 
								# Create arrays of Boolean values indicating the outlier rows
								upper_array = np.where(test_df['Following']>=upper)[0]
								lower_array = np.where(test_df['Following']<=lower)[0]
								
								print('upper bound:\n', test_df.loc[upper_array, 'Following'])
								print('lower bound:\n', test_df.loc[lower_array, 'Following'])
								
							</code>
						</pre>						
                        <!-- <div>
                            Normalization []
                        </div> -->
						<pre>
							<code class="language-python">
								# Normalization/standardization/scaling FOR FOLLOWING AND FOLLOWERS

								df['Following (normalized)'] = (df['Following'] - df['Following'].mean()) / df['Following'].std()
								df['Followers (normalized)'] = (df['Followers'] - df['Followers'].mean()) / df['Followers'].std()
								#print(df['Following (normalized)'])
								#print(df['Followers (normalized)'])
								
								test_df['Following (normalized)'] = (test_df['Following'] - test_df['Following'].mean()) / test_df['Following'].std()
								test_df['Followers (normalized)'] = (test_df['Followers'] - test_df['Followers'].mean()) / test_df['Followers'].std()
								print(test_df['Following (normalized)'])
								print(test_df['Followers (normalized)'])
							</code>
						</pre>
                        <div>
                            <p>Since we have categorical data that will be compared to numerical data, we had to use <strong>categorical data encoding</strong>.</p>
							<p>For the Account type, we have three categories: Identified, Media, and Anonymous.
								In order to label the Account Type, we divide them into 2 types: 1 for known which includes Media and Identified; 0 for unknown which includes Anonymous.</p>
                        </div>
						<pre>
							<code class="language-python">
								# Categorical data encoding for ACCOUNT TYPE
								def categorize(x):
								  if x == "Anonymous":
									return 0 #uknown
								  elif x == "Media" or x == "Identified":
									return 1 #known
								
								df['Account type (categorized)'] = df['Account type'].apply(lambda x: categorize(x))
								test_df['Account type (categorized)'] = test_df['Account type'].apply(lambda x: categorize(x))
								print(test_df['Account type (categorized)'])
							</code>
						</pre>
					</div>
				</div>
			</article>

			<article id="nlp" class="wrapper style3">
				<div class="container">
					<header>
						<h2>Let's move on to natural language processing.</h2>
						<p>The following are the tasks we did for NLP:</p>
					</header>
					<div class="row aln-left" style="text-align: left;">
                        <div>
                            The first task is <strong>removing the emojis and emoticons</strong> in the 'Tweet' column. This is done by storing emojis and emoticons in their respective database then using functions to take in the string and replace occurences of emojis/emoticons with their textual descriptions. The function finds all occurrences of emojis/emoticons in the text and then replaces them with the corresponding textual description by looking up the emoji/emoticon in the appropriate dataframe.  
                        </div>
						<pre>
							<code class="language-python">
								# Handle Emojis 
								url_emoji = "https://drive.google.com/uc?id=1G1vIkkbqPBYPKHcQ8qy0G2zkoab2Qv4v"
								df_emoji = pd.read_pickle(url_emoji)
								df_emoji = {v: k for k, v in df_emoji.items()}
								
								def emoji_to_word(text):
								  for emot in df_emoji:
									text = re.sub(r'('+emot+')', "_".join(df_emoji[emot].replace(",","").replace(":","").split()), text)
								  return text
								
								# Handle Emoticons 
								url_emote = "https://drive.google.com/uc?id=1HDpafp97gCl9xZTQWMgP2kKK_NuhENlE"
								df_emote = pd.read_pickle(url_emote)
								
								def emote_to_word(text):
									for emot in df_emote:
										text = re.sub(u'('+emot+')', "_".join(df_emote[emot].replace(",","").split()), text)
										text = text.replace("<3", "heart" ) # not included in emoticons database
									return text
								
								df['Tweet'] = df['Tweet'].apply(lambda x: emoji_to_word(x))
								df['Tweet'] = df['Tweet'].apply(lambda x: emote_to_word(x))
							</code>
						</pre>

						<div>
							The text in the 'Tweet' column is <strong>converted to lowercase</strong> using .str.lower. Punctuations are also removed by using .str.translate(str.maketrans('', '', string.punctuation)). Converted strings are stored in the 'Text' column.   
						</div>
						<pre>
							<code class="language-python">
								# convert to lowercase
								df['Text'] = df['Tweet'].str.lower()
								
								# remove punctuation 
								df['Text'] = df['Text'].str.translate(str.maketrans('', '', string.punctuation)) # di ko alam if gumana toh
							
							</code>
						</pre>

						<div> 
							<strong>Tokenizing</strong> the string in the 'Tweet' column is done by using .word_tokenize from the nltk library. This is to basically divide the string into elements or <em>tokens</em>. Tokenized strings are stored in the 'Tokens' column.
						</div>
						<pre>
							<code class="language-python">
								# tokenize 
								df['Tokens'] = df['Text'].apply(nltk.word_tokenize)
								
							</code>
						</pre>

						<div>
							After tokenizing, we <strong>removed stopwords</strong> using stopwords from nltk.corpus. We did this by making a function and only retaining tokens that are not in the collection of stopwords.     	
						</div>
						<pre>
							<code class="language-python">
								# Removing stop words
								from nltk.corpus import stopwords
								
								def remove_stopwords(tokens):
									return [token for token in tokens if token.lower() not in stopwords.words('english')]
								
								df['Tokens'] = df['Tokens'].apply(remove_stopwords)
							</code>
						</pre>

						<div> 
							Lastly, <strong>stemming and lemmatization</strong>. We used PorterStemmer() and WordNetLemmatizer() from nltk.stem for this task, then stored the values in the 'Stemmed' and 'Lemmatized' columns. 
						</div>
						<pre>
							<code class="language-python">
								# Stemming and Lemmatization
								from nltk.stem import PorterStemmer, WordNetLemmatizer
								
								stemmer = PorterStemmer()
								lemmatizer = WordNetLemmatizer()
								
								def stem_tokens(tokens):
									return [stemmer.stem(token) for token in tokens]
								
								# define a lemmatizer function
								def lemmatize_tokens(tokens):
									return [lemmatizer.lemmatize(token) for token in tokens]
								
								df['Stemmed'] = df['Tokens'].apply(stem_tokens)
								df['Lemmatized'] = df['Tokens'].apply(lemmatize_tokens)
							</code>
						</pre>
						<div> 
							These steps were all done so that if we decided to also analyze the tweets and their content, we can analyze and process it at a more granular level. The NLP algorithms that may be used can now operate on individual tokens rather than on entire texts. To add, removing stopwords also help in reducing the dimensionality of the data and focus on the more informative words in the text.
						</div>						
					</div>
				</div>
			</article>			

			<article id="tsa" class="wrapper style3">
				<div class="container">
					<header>
						<h2>Then we have the time series analysis</h2>
						<p>These are the steps we took to preprocess our dataset for data exploration:</p>
					</header>
					<div class="row aln-left" style="text-align: left;">
						<div> <!--class="col-6 col-6-medium col-12-small"-->
                            For the time series analysis,
						</div>
						<pre>
							<code class="language-python">

							</code>
						</pre>

					</div>
					<footer>
						<p>Want to learn more? Take a look at our source codes.</p>
						<a href="https://github.com/cs132-g26/cs132-g26.github.io.git" class="button large scrolly">Github Repo</a>
						<p>Or, continue to look at our plots and graphs.</p>
						<a href="visualization.html" class="button large scrolly">Visualization</a>
					</footer>
				</div>
			</article>			

		<!-- Team -->
			<article id="team" class="wrapper style4">
				<div class="container medium">
					<header>
						<h2>We'd like to hear from you.</h2>
						<p>You can add more information about the team members here.</p>
					</header>
					<div class="row">
						<div class="col-12">
							<form method="post" action="#">
								<div class="row">
									<div class="col-6 col-12-small">
										<input type="text" name="name" id="name" placeholder="Name" />
									</div>
									<div class="col-6 col-12-small">
										<input type="text" name="email" id="email" placeholder="Email" />
									</div>
									<div class="col-12">
										<input type="text" name="subject" id="subject" placeholder="Subject" />
									</div>
									<div class="col-12">
										<textarea name="message" id="message" placeholder="Message"></textarea>
									</div>
									<div class="col-12">
										<ul class="actions">
											<li><input type="submit" value="Send Message" /></li>
											<li><input type="reset" value="Clear Form" class="alt" /></li>
										</ul>
									</div>
								</div>
							</form>
						</div>
						<div class="col-12">
							<hr />
							<h3>Find us on ...</h3>
							<ul class="social">
								<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
								<li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
								<li><a href="#" class="icon brands fa-tumblr"><span class="label">Tumblr</span></a></li>
								<li><a href="#" class="icon brands fa-google-plus"><span class="label">Google+</span></a></li>
								<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
								<!--
								<li><a href="#" class="icon solid fa-rss"><span>RSS</span></a></li>
								<li><a href="#" class="icon brands fa-instagram"><span>Instagram</span></a></li>
								<li><a href="#" class="icon brands fa-foursquare"><span>Foursquare</span></a></li>
								<li><a href="#" class="icon brands fa-skype"><span>Skype</span></a></li>
								<li><a href="#" class="icon brands fa-soundcloud"><span>Soundcloud</span></a></li>
								<li><a href="#" class="icon brands fa-youtube"><span>YouTube</span></a></li>
								<li><a href="#" class="icon brands fa-blogger"><span>Blogger</span></a></li>
								<li><a href="#" class="icon brands fa-flickr"><span>Flickr</span></a></li>
								<li><a href="#" class="icon brands fa-vimeo"><span>Vimeo</span></a></li>
								-->
							</ul>
							<hr />
						</div>
					</div>
					<footer>
						<ul id="copyright">
							<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>
				</div>
			</article>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/prism.js"></script>
			<script src="assets/js/prism-python.min.js"></script>

	</body>
</html>
